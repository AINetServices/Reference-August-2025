"""
Multi-agent workflow using LangGraph for processing resumes and extracting references.
Implements a comprehensive pipeline with custom tools and state management.
"""

import os
import json
from typing import Dict, List, Any, TypedDict
from datetime import datetime
import requests
from io import BytesIO

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import create_react_agent
from langchain_groq import ChatGroq
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.tools import tool
from langchain.agents import AgentExecutor

from .tools.resume_tools import ResumeParserTool, ReferenceExtractorTool
from .tools.database_tools import QuestionFetcherTool
from .tools.vector_tools import VectorStoreTool

class WorkflowState(TypedDict):
    """State structure for the multi-agent workflow"""
    resume_url: str
    role: str
    organization: str
    resume_text: str
    applicant_info: Dict[str, Any]
    references: List[Dict[str, Any]]
    questions: List[str]
    vectorstore_path: str
    error_message: str
    status: str

class ReferenceCheckingWorkflow:
    """
    Multi-agent workflow orchestrator using LangGraph.
    Manages the complete pipeline from resume processing to reference extraction.
    """

    def __init__(self):  # ‚Üê FIXED: Proper indentation
        self.llm = ChatGroq(
            model="llama-3.3-70b-versatile",#Updated model
            api_key=os.getenv("GROQ_API_KEY")
        )
        
        # Use a simpler embedding approach - we'll skip embeddings for now to avoid the import issue
        self.embeddings = None  # We'll handle this differently
        
        # Initialize tools
        self.resume_parser = ResumeParserTool(llm=self.llm)
        self.reference_extractor = ReferenceExtractorTool(llm=self.llm)
        self.question_fetcher = QuestionFetcherTool()
        self.vector_tool = VectorStoreTool()

        # Build the workflow graph
        self.workflow = self._build_workflow()

    def _build_workflow(self) -> StateGraph:
        """Build the multi-agent workflow using LangGraph"""

        # Create the state graph
        workflow = StateGraph(WorkflowState)

        # Add nodes
        workflow.add_node("download_resume", self._download_resume_node)
        workflow.add_node("parse_resume", self._parse_resume_node)
        workflow.add_node("extract_applicant", self._extract_applicant_node)
        workflow.add_node("extract_references", self._extract_references_node)
        workflow.add_node("build_vectorstore", self._build_vectorstore_node)
        workflow.add_node("fetch_questions", self._fetch_questions_node)
        workflow.add_node("finalize", self._finalize_node)

        # Add edges to define the flow
        workflow.add_edge("download_resume", "parse_resume")
        workflow.add_edge("parse_resume", "extract_applicant")
        workflow.add_edge("extract_applicant", "extract_references")
        workflow.add_edge("extract_references", "build_vectorstore")
        workflow.add_edge("build_vectorstore", "fetch_questions")
        workflow.add_edge("fetch_questions", "finalize")
        workflow.add_edge("finalize", END)

        # Set entry point
        workflow.set_entry_point("download_resume")

        return workflow.compile()

    async def _download_resume_node(self, state: WorkflowState) -> WorkflowState:
        """Download and extract text from resume URL"""
        try:
            print(f"üì• Downloading resume from: {state['resume_url']}")
            response = requests.get(state["resume_url"])
            response.raise_for_status()

            # Use resume parser to extract text - with error handling
            try:
                resume_text = self.resume_parser.parse_file_content(
                    BytesIO(response.content),
                    state["resume_url"]
                )
            except Exception as parse_error:
                print(f"‚ö†Ô∏è Parser failed, using fallback: {parse_error}")
                # Fallback: use direct PDF parsing
                import pdfplumber
                resume_text = ""
                with pdfplumber.open(BytesIO(response.content)) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            resume_text += page_text + "\n"

            if not resume_text.strip():
                raise Exception("No text could be extracted from resume")

            state["resume_text"] = resume_text
            state["status"] = "downloaded"
            print(f"‚úÖ Resume downloaded and parsed, text length: {len(resume_text)}")

        except Exception as e:
            error_msg = f"Failed to download resume: {str(e)}"
            print(f"‚ùå {error_msg}")
            state["error_message"] = error_msg
            state["status"] = "error"

        return state

    async def _parse_resume_node(self, state: WorkflowState) -> WorkflowState:
        """Parse resume text into structured chunks"""
        try:
            if state["status"] == "error":
                return state

            # Split text into manageable chunks
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", ". ", " "]
            )

            chunks = text_splitter.split_text(state["resume_text"])
            state["resume_chunks"] = chunks
            state["status"] = "parsed"
            print(f"‚úÖ Resume parsed into {len(chunks)} chunks")

        except Exception as e:
            state["error_message"] = f"Failed to parse resume: {str(e)}"
            state["status"] = "error"

        return state

    async def _extract_applicant_node(self, state: WorkflowState) -> WorkflowState:
        """Extract applicant information using LLM agent"""
        try:
            if state["status"] == "error":
                return state

            # Use direct LLM call instead of agent to avoid empty messages issue
            applicant_info = await self._extract_applicant_info_direct(state["resume_text"])
            state["applicant_info"] = applicant_info
            state["status"] = "applicant_extracted"
            print(f"‚úÖ Applicant info extracted: {applicant_info.get('full_name', 'Unknown')}")

        except Exception as e:
            print(f"‚ö†Ô∏è Applicant extraction failed: {str(e)}")
            # Don't fail the workflow - just use empty applicant info
            state["applicant_info"] = {
                "full_name": "Unknown",
                "email": "Unknown", 
                "phone": "Unknown",
                "current_position": "Unknown",
                "experience_years": "Unknown",
                "key_skills": []
            }
            state["status"] = "applicant_extracted"

        return state

    async def _extract_references_node(self, state: WorkflowState) -> WorkflowState:
        """Extract reference information using specialized agent"""
        try:
            if state["status"] == "error":
                return state

            # Use direct LLM call instead of agent to avoid empty messages issue
            references = await self._extract_references_direct(state["resume_text"], state["role"])
            
            # Always set references, even if empty
            state["references"] = references
            state["status"] = "references_extracted"
            
            print(f"üìù References node completed: {len(references)} references found")

        except Exception as e:
            print(f"‚ö†Ô∏è Reference extraction failed, but continuing: {str(e)}")
            # Don't set error status - just use empty references
            state["references"] = []
            state["status"] = "references_extracted"  # Still mark as completed

        return state

    async def _build_vectorstore_node(self, state: WorkflowState) -> WorkflowState:
        """Build vector store for semantic search - Simplified version"""
        try:
            if state["status"] == "error":
                return state

            # For now, we'll skip the actual vector store creation to avoid embedding issues
            # This is a temporary simplification to get the main workflow working
            vectorstore_path = f"./vectorstore/{state['role']}_{state['organization']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # Just create the directory structure without actual embeddings
            os.makedirs(vectorstore_path, exist_ok=True)

            state["vectorstore_path"] = vectorstore_path
            state["status"] = "vectorstore_built"
            print(f"‚úÖ Vectorstore path created: {vectorstore_path}")

        except Exception as e:
            print(f"‚ö†Ô∏è Vectorstore creation failed, but continuing: {str(e)}")
            # Don't fail the workflow for vectorstore issues
            state["vectorstore_path"] = ""
            state["status"] = "vectorstore_built"

        return state

    async def _fetch_questions_node(self, state: WorkflowState) -> WorkflowState:
        """Fetch predefined questions for the role and organization"""
        try:
            if state["status"] == "error":
                return state

            # Fetch questions from database
            questions = await self.question_fetcher.get_questions(
                state["role"],
                state["organization"]
            )

            state["questions"] = questions
            state["status"] = "questions_fetched"
            print(f"‚úÖ Questions fetched: {len(questions)} questions")

        except Exception as e:
            print(f"‚ö†Ô∏è Question fetching failed, but continuing: {str(e)}")
            # Don't fail the workflow for question fetching issues
            state["questions"] = []
            state["status"] = "questions_fetched"

        return state

    async def _finalize_node(self, state: WorkflowState) -> WorkflowState:
        """Finalize the workflow and prepare results"""
        try:
            if state["status"] == "error":
                return state

            state["status"] = "completed"
            print("‚úÖ Workflow completed successfully")

        except Exception as e:
            state["error_message"] = f"Failed to finalize: {str(e)}"
            state["status"] = "error"

        return state

    async def _extract_applicant_info_direct(self, resume_text: str) -> Dict[str, Any]:
        """Extract applicant information using direct LLM call (fixes agent issues)"""
        try:
            prompt = PromptTemplate.from_template("""
            Extract the following applicant information from this resume text:

            Resume Text:
            {resume_text}

            Extract and return as valid JSON only (no other text):
            {{
                "full_name": "applicant's full name",
                "email": "email address", 
                "phone": "phone number",
                "current_position": "current job title",
                "experience_years": "estimated years of experience",
                "key_skills": ["skill1", "skill2", "skill3"]
            }}

            If any information is not found, use "Unknown" as the value.
            Return only the JSON object, no additional text.
            """)

            # Limit text length to avoid token limits
            limited_text = resume_text[:3000] + "..." if len(resume_text) > 3000 else resume_text
            
            result = self.llm.invoke(prompt.format(resume_text=limited_text))
            
            # Parse the JSON response
            content = result.content.strip()
            start_idx = content.find('{')
            end_idx = content.rfind('}') + 1
            
            if start_idx >= 0 and end_idx > start_idx:
                json_str = content[start_idx:end_idx]
                return json.loads(json_str)
            else:
                raise Exception("No JSON found in response")
                
        except Exception as e:
            print(f"‚ùå Direct applicant extraction failed: {e}")
            return {
                "full_name": "Unknown",
                "email": "Unknown",
                "phone": "Unknown", 
                "current_position": "Unknown",
                "experience_years": "Unknown",
                "key_skills": []
            }

    async def _extract_references_direct(self, resume_text: str, role: str) -> List[Dict[str, Any]]:
        """Extract reference information using direct LLM call (fixes agent issues)"""
        try:
            prompt = PromptTemplate.from_template("""
            Extract reference information from this resume for a {role} position:

            Resume Text:
            {resume_text}

            Look for reference contacts including:
            - Previous supervisors, managers, or colleagues
            - Contact information (email, phone)
            - Company/organization names
            - Working relationship

            Return as a valid JSON array only (no other text). Example:
            [
                {{
                    "name": "Reference Name",
                    "email": "email@example.com",
                    "company": "Company Name", 
                    "relationship": "Direct supervisor",
                    "years_worked": "2 years",
                    "context": "Worked together on Project X"
                }}
            ]

            If no references are found, return an empty array: []

            Return only the JSON array, no additional text.
            """)

            # Limit text length to avoid token limits
            limited_text = resume_text[:3000] + "..." if len(resume_text) > 3000 else resume_text
            
            result = self.llm.invoke(prompt.format(resume_text=limited_text, role=role))
            
            # Parse the JSON response
            content = result.content.strip()
            start_idx = content.find('[')
            end_idx = content.rfind(']') + 1
            
            if start_idx >= 0 and end_idx > start_idx:
                json_str = content[start_idx:end_idx]
                references = json.loads(json_str)
                
                # Validate references
                valid_references = []
                for ref in references:
                    if isinstance(ref, dict) and ref.get("name"):
                        valid_references.append({
                            "name": ref.get("name", "Unknown"),
                            "email": ref.get("email", ""),
                            "company": ref.get("company", ""),
                            "relationship": ref.get("relationship", "Professional contact"),
                            "years_worked": ref.get("years_worked", ""),
                            "context": ref.get("context", "")
                        })
                
                return valid_references
            else:
                # Check if the response indicates no references
                if any(term in content.lower() for term in ["no references", "[]", "none", "not found"]):
                    return []
                else:
                    raise Exception("No valid JSON array found in response")
                    
        except Exception as e:
            print(f"‚ùå Direct reference extraction failed: {e}")
            return []

    def _parse_applicant_info(self, llm_output: str) -> Dict[str, Any]:
        """Parse LLM output into structured applicant information"""
        try:
            # Try to extract JSON from the output
            start_idx = llm_output.find('{')
            end_idx = llm_output.rfind('}') + 1

            if start_idx >= 0 and end_idx > start_idx:
                json_str = llm_output[start_idx:end_idx]
                return json.loads(json_str)
            else:
                # Fallback: create basic structure
                return {
                    "full_name": "Unknown",
                    "email": "Unknown",
                    "phone": "Unknown",
                    "current_position": "Unknown",
                    "experience_years": "Unknown",
                    "key_skills": []
                }
        except json.JSONDecodeError:
            return {"error": "Failed to parse applicant information"}

    def _parse_references(self, llm_output: str) -> List[Dict[str, Any]]:
        """Parse LLM output into structured reference information"""
        try:
            # Handle empty or None output
            if not llm_output or llm_output.strip() == "":
                print("‚ö†Ô∏è No references found in resume - returning empty list")
                return []

            # Try to extract JSON from the output
            start_idx = llm_output.find('[')
            end_idx = llm_output.rfind(']') + 1

            if start_idx >= 0 and end_idx > start_idx:
                json_str = llm_output[start_idx:end_idx]
                references = json.loads(json_str)

                # Validate and clean references
                cleaned_refs = []
                for ref in references:
                    if isinstance(ref, dict) and ref.get("name"):
                        cleaned_refs.append({
                            "name": ref.get("name", "Unknown"),
                            "email": ref.get("email", "Not provided"),
                            "company": ref.get("company", "Unknown"),
                            "relationship": ref.get("relationship", "Professional contact"),
                            "years_worked": ref.get("years_worked", "Unknown"),
                            "context": ref.get("context", "")
                        })

                print(f"‚úÖ Found {len(cleaned_refs)} references")
                return cleaned_refs
            else:
                # If no JSON array found, check for specific patterns indicating no references
                no_ref_patterns = [
                    "no references", 
                    "no reference", 
                    "[]",
                    "none found",
                    "not provided"
                ]
                
                if any(pattern in llm_output.lower() for pattern in no_ref_patterns):
                    print("‚ö†Ô∏è LLM explicitly stated no references found")
                    return []
                else:
                    print(f"‚ö†Ô∏è Could not parse references from: {llm_output[:200]}...")
                    return []

        except json.JSONDecodeError as e:
            print(f"‚ùå JSON decode error in references: {e}")
            return []
        except Exception as e:
            print(f"‚ùå Unexpected error parsing references: {e}")
            return []

    async def run_workflow(self, resume_url: str, role: str, organization: str) -> Dict[str, Any]:
        """
        Run the complete multi-agent workflow
        """
        print(f"üéØ Workflow started for {role} at {organization}")
        
        try:
            initial_state = WorkflowState(
                resume_url=resume_url,
                role=role,
                organization=organization,
                resume_text="",
                applicant_info={},
                references=[],
                questions=[],
                vectorstore_path="",
                error_message="",
                status="initialized"
            )

            # Execute the workflow
            final_state = await self.workflow.ainvoke(initial_state)

            # Debug the final state
            print(f"üìä Final workflow state:")
            print(f"   Status: {final_state.get('status')}")
            print(f"   References found: {len(final_state.get('references', []))}")
            print(f"   Error message: {final_state.get('error_message')}")
            
            # Always return a valid result, even if empty
            result = {
                "applicant_info": final_state.get("applicant_info", {}),
                "references": final_state.get("references", []),
                "questions": final_state.get("questions", []),
                "vectorstore_path": final_state.get("vectorstore_path", ""),
                "status": final_state.get("status", "unknown"),
                "error_message": final_state.get("error_message", "")
            }
            
            # If no references found, that's OK - don't return error
            if not result["references"]:
                print("‚ÑπÔ∏è No references found - this is acceptable")
                result["references"] = []  # Ensure it's always an empty list, not an error
            
            return result
            
        except Exception as e:
            print(f"üí• Workflow execution failed: {str(e)}")
            # Return a valid result even on complete failure
            return {
                "applicant_info": {},
                "references": [],
                "questions": [],
                "vectorstore_path": "",
                "status": "error", 
                "error_message": f"Workflow execution failed: {str(e)}"
            }